{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bb892c",
   "metadata": {
    "id": "f7bb892c"
   },
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8001a2b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "8001a2b4",
    "outputId": "4ca29dc7-e80e-4d54-82fe-7b5dc99ea8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 3969\n",
      "Test size 997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emergency Room Reports</td>\n",
       "      <td>REASON FOR THE VISIT:,  Very high PT/INR.,HIST...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>NAME OF PROCEDURE,1.  Selective coronary angio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         medical_specialty                                      transcription  \\\n",
       "0   Emergency Room Reports  REASON FOR THE VISIT:,  Very high PT/INR.,HIST...   \n",
       "1                  Surgery  PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...   \n",
       "2                  Surgery  NAME OF PROCEDURE,1.  Selective coronary angio...   \n",
       "\n",
       "   labels  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"new_test.csv\", index_col=0)\n",
    "\n",
    "print(\"Train size\", len(train_df))\n",
    "print(\"Test size\", len(test_df))\n",
    "train_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326d130",
   "metadata": {
    "id": "b326d130"
   },
   "source": [
    "### Train Set Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c349df00",
   "metadata": {
    "id": "c349df00",
    "outputId": "88c05625-d53a-431a-9044-702d88ae9a36",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Surgery                          863\n",
       " Consult - History and Phy.       410\n",
       " Cardiovascular / Pulmonary       309\n",
       " Orthopedic                       289\n",
       " Radiology                        213\n",
       " General Medicine                 209\n",
       " Gastroenterology                 176\n",
       " Neurology                        170\n",
       " SOAP / Chart / Progress Notes    135\n",
       " Urology                          134\n",
       " Obstetrics / Gynecology          123\n",
       " Discharge Summary                 87\n",
       " ENT - Otolaryngology              82\n",
       " Neurosurgery                      71\n",
       " Hematology - Oncology             68\n",
       " Ophthalmology                     67\n",
       " Emergency Room Reports            63\n",
       " Nephrology                        63\n",
       " Pediatrics - Neonatal             55\n",
       " Pain Management                   54\n",
       " Psychiatry / Psychology           45\n",
       " Office Notes                      38\n",
       " Podiatry                          35\n",
       " Dermatology                       21\n",
       " Dentistry                         21\n",
       " Cosmetic / Plastic Surgery        19\n",
       " Letters                           19\n",
       " Endocrinology                     16\n",
       " Physical Medicine - Rehab         16\n",
       " Bariatrics                        15\n",
       " IME-QME-Work Comp etc.            12\n",
       " Chiropractic                      12\n",
       " Sleep Medicine                    12\n",
       " Diets and Nutritions               9\n",
       " Speech - Language                  8\n",
       " Autopsy                            7\n",
       " Hospice - Palliative Care          6\n",
       " Allergy / Immunology               6\n",
       " Rheumatology                       6\n",
       " Lab Medicine - Pathology           5\n",
       "Name: medical_specialty, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"medical_specialty\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8c8f3",
   "metadata": {
    "id": "9df8c8f3"
   },
   "source": [
    "### Sample Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4b315a",
   "metadata": {
    "id": "4b4b315a",
    "outputId": "fc59a11b-9681-4921-eb84-67a14739c4fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('REASON FOR THE VISIT:,  Very high PT/INR.,HISTORY: , The patient is an '\n",
      " '81-year-old lady whom I met last month when she came in with pneumonia and '\n",
      " 'CHF.  She was noticed to be in atrial fibrillation, which is a chronic '\n",
      " 'problem for her.  She did not want to have Coumadin started because she said '\n",
      " 'that she has had it before and the INR has had been very difficult to '\n",
      " 'regulate to the point that it was dangerous, but I convinced her to restart '\n",
      " 'the Coumadin again.  I gave her the Coumadin as an outpatient and then the '\n",
      " 'INR was found to be 12.  So, I told her to come to the emergency room to get '\n",
      " 'vitamin K to reverse the anticoagulation.,PAST MEDICAL HISTORY:,1.  '\n",
      " 'Congestive heart failure.,2.  Renal insufficiency.,3.  Coronary artery '\n",
      " 'disease.,4.  Atrial fibrillation.,5.  COPD.,6.  Recent pneumonia.,7.  '\n",
      " 'Bladder cancer.,8.  History of ruptured colon.,9.  Myocardial '\n",
      " 'infarction.,10.  Hernia repair.,11.  Colon resection.,12.  Carpal tunnel '\n",
      " 'repair.,13.  Knee surgery.,MEDICATIONS:,1.  Coumadin.,2.  Simvastatin.,3.  '\n",
      " 'Nitrofurantoin.,4.  Celebrex.,5.  Digoxin.,6.  Levothyroxine.,7.  '\n",
      " 'Vicodin.,8.  Triamterene and hydrochlorothiazide.,9.  Carvedilol.,SOCIAL '\n",
      " 'HISTORY:  ,She does not smoke and she does not drink.,PHYSICAL '\n",
      " 'EXAMINATION:,GENERAL:  Lady in no distress.,VITAL SIGNS:  Blood pressure '\n",
      " '100/46, pulse of 75, respirations 12, and temperature 98.2.,HEENT:  Head is '\n",
      " 'normal.,NECK:  Supple.,LUNGS:  Clear to auscultation and percussion.,HEART:  '\n",
      " 'No S3, no S4, and no murmurs.,ABDOMEN:  Soft.,EXTREMITIES:  Lower '\n",
      " 'extremities, no edema.,ASSESSMENT:,1.  Atrial fibrillation.,2.  '\n",
      " 'Coagulopathy, induced by Coumadin.,PLAN: , Her INR at the office was 12.  I '\n",
      " 'will repeat it, and if it is still elevated, I will give vitamin K 10 mg in '\n",
      " '100 mL of D5W and then send her home and repeat the PT/INR next week.  I '\n",
      " 'believe at this time that it is too risky to use Coumadin in her case '\n",
      " 'because of her age and comorbidities, the multiple medications that she '\n",
      " 'takes and it is very difficult to keep an adequate level of anticoagulation '\n",
      " 'that is safe for her.  She is prone to a fall and this would be a big '\n",
      " 'problem.  We will use one aspirin a day instead of the anticoagulation.  She '\n",
      " 'is aware of the risk of stroke, but she is very scared of the '\n",
      " 'anticoagulation with Coumadin and does not want to use the Coumadin at this '\n",
      " 'time and I understand.  We will see her as an outpatient.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(train_df.transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20467d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b6f4d",
   "metadata": {
    "id": "0f1b6f4d"
   },
   "source": [
    "### Sample Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68977658",
   "metadata": {
    "id": "68977658"
   },
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e142223",
   "metadata": {
    "id": "3e142223"
   },
   "outputs": [],
   "source": [
    "unique_classes = train_df[\"medical_specialty\"].unique()\n",
    "\n",
    "idx_2_class = {i: s for i, s in enumerate(unique_classes)}\n",
    "class_2_idx = {s: i for i, s in enumerate(unique_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac5d380",
   "metadata": {
    "id": "cac5d380"
   },
   "outputs": [],
   "source": [
    "train_df[\"labels\"] = train_df[\"medical_specialty\"].apply(lambda s: class_2_idx[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233a4365",
   "metadata": {
    "id": "233a4365"
   },
   "outputs": [],
   "source": [
    "train_train_df, train_test_df = \\\n",
    "    train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449f16bd",
   "metadata": {
    "id": "449f16bd"
   },
   "outputs": [],
   "source": [
    "ds_dict = {\n",
    "    'train': Dataset.from_pandas(train_train_df),\n",
    "    'val': Dataset.from_pandas(train_test_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "}\n",
    "\n",
    "ds = DatasetDict(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a995f402",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "3a6ee68ccb5844bf8a7ef1736274e442",
      "12d72b7eadcf485aa898b4fcb9c89d60",
      "00307d96c1f74a10a7447c60d27b3e47"
     ]
    },
    "id": "a995f402",
    "outputId": "3830df1e-a840-4531-efa4-afd25004df66",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2031e49d051c4bd285deada246e0c0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed19ec8bbd145cb85e021620885edbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e33c8e39b8406ab17397564050952a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_text(texts):\n",
    "    return tokenizer(texts[\"transcription\"], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(tokenize_text, batched=True)\n",
    "ds[\"val\"] = ds[\"val\"].map(tokenize_text, batched=True)\n",
    "ds[\"test\"] = ds[\"test\"].map(tokenize_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6c4319",
   "metadata": {
    "id": "fa6c4319",
    "outputId": "9dc28ef0-00a2-476c-bfe0-d1c9d469eb59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ca876",
   "metadata": {
    "id": "e22ca876"
   },
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f26fe53",
   "metadata": {
    "id": "1f26fe53"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414f5205",
   "metadata": {
    "id": "414f5205"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "logging_steps = len(train_train_df) // batch_size\n",
    "output_dir = \"hf_trainer\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "     num_train_epochs=5,\n",
    "     learning_rate=2e-5,\n",
    "     per_device_train_batch_size=batch_size,\n",
    "     per_device_eval_batch_size=batch_size,\n",
    "     weight_decay=0.01,\n",
    "     evaluation_strategy=\"epoch\",\n",
    "     logging_steps=logging_steps,\n",
    "     push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "994876be",
   "metadata": {
    "id": "994876be"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['val'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8cfca94",
   "metadata": {
    "id": "c8cfca94",
    "outputId": "75e80ad0-c61f-4b2f-a474-c43638fca0b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: medical_specialty, transcription, __index_level_0__. If medical_specialty, transcription, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\InstalledUtilities\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2778\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 435\n",
      "  Number of trainable parameters = 66984232\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 31/435 05:37 < 1:18:21, 0.09 it/s, Epoch 0.34/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21688\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\InstalledUtilities\\Anaconda\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m         )\n\u001b[1;32m-> 1543\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\InstalledUtilities\\Anaconda\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1789\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m                 if (\n",
      "\u001b[1;32mC:\\InstalledUtilities\\Anaconda\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2555\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2557\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2559\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\InstalledUtilities\\Anaconda\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mC:\\InstalledUtilities\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6603a",
   "metadata": {
    "id": "e0a6603a"
   },
   "source": [
    "### Making Inference on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc4a1d",
   "metadata": {
    "id": "d3bc4a1d",
    "outputId": "dd0478e2-0a9d-4048-c6d4-8cdd4da2dd63"
   },
   "outputs": [],
   "source": [
    "ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1f4da",
   "metadata": {
    "id": "d0d1f4da",
    "outputId": "ea2c870e-edbf-4e0b-ff25-23e37e9223ad"
   },
   "outputs": [],
   "source": [
    "pred_y = trainer.predict(ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20da194",
   "metadata": {
    "id": "0411c232"
   },
   "outputs": [],
   "source": [
    "a = pd.Series(pred_y.predictions.argmax(axis=1))\n",
    "a.name = \"Expected\"\n",
    "a.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42973b0",
   "metadata": {},
   "source": [
    "### Normalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    'new_train.csv',\n",
    "    encoding='utf-8', \n",
    "    engine='python'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce140c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters (non alphabetical char)\n",
    "import re\n",
    "# mystring = \"      Ballenger       Center            Dr # 351     \"\n",
    "# mystring = re.sub('[^A-Za-z0-9]+ ', ' ', mystring).strip()\n",
    "# mystring\n",
    "# df['a'] = df['a'].apply(lambda x: x + 1)\n",
    "# bank_accounts['Address Line 1'] = bank_accounts['Address Line 1'].apply(lambda x: re.sub('[^A-Za-z0-9]+ ', ' ', x).strip())\n",
    "# bank_accounts.head()\n",
    "train_df['transcription'] = train_df['transcription'].apply(lambda x: re.sub('[^A-Za-z]+ ', ' ', x).strip())\n",
    "# train_df['transcription'] = train_df['transcription'].apply(lambda x: re.sub('the ', '', x).strip())\n",
    "# train_df['transcription'] = train_df['transcription'].apply(lambda x: re.sub('and ', '', x).strip())\n",
    "# train_df['transcription'] = train_df['transcription'].apply(lambda x: re.sub('is ', '', x).strip())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ab1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "# load in the spell checker library\n",
    "spell = SpellChecker()\n",
    "\n",
    "# function to check the spelling of a given text\n",
    "def check_spelling(text):\n",
    "    # convert the text to lowercase for comparison\n",
    "    text = text.lower()\n",
    "\n",
    "    # this is where we will store all the words with the wrong words corrected\n",
    "    corrected_text = []\n",
    "\n",
    "    # correct each word in the text and add it to corrected_text\n",
    "    for word in text.split():\n",
    "        # spell.correction corrects the spelling of a given word by comparing it to a dictionary of correctly spelt words\n",
    "        corrected_word = spell.correction(word)\n",
    "        corrected_text.append(corrected_word)\n",
    "    \n",
    "    # join together the words in the corrected_text array into a singular string and seperate each word with a space\n",
    "    text = \" \".join(corrected_text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23ba5701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poop\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords & Tokenization\n",
    "# https://levelup.gitconnected.com/how-to-remove-stopwords-from-text-in-python-9e9fbfcbca8d\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# stopwords.words('english')\n",
    "\n",
    "# # my_txt = \"I'm George. I live in Athens! This is my blog, hopefully you enjoy this post! Look at this!\"\n",
    "# filtered_list = []\n",
    "# stop_words = nltk.corpus.stopwords.words('english')\n",
    "# # Tokenize the sentence\n",
    "# # words = word_tokenize(my_txt)\n",
    "# words = word_tokenize(train_df['transcription'])\n",
    "# for w in words:\n",
    "#     if w.lower() not in stop_words:\n",
    "#         filtered_list.append(w)\n",
    "        \n",
    "# filtered_list\n",
    "# my_clean_txt = \" \".join(filtered_list)\n",
    "# my_clean_txt\n",
    "\n",
    "\n",
    "# stop_words = set(stopwords.words('english')) \n",
    "# file1 = train_df['transcription']\n",
    "  \n",
    "# # Use this to read file content as a stream: \n",
    "# # line = file1.read()\n",
    "# words = line.split() \n",
    "# for r in words: \n",
    "#     if not r in stop_words: \n",
    "#         appendFile = open('filteredtext.txt','a') \n",
    "#         appendFile.write(\" \"+r) \n",
    "#         appendFile.close() \n",
    "\n",
    "\n",
    "import io \n",
    "import codecs\n",
    "import csv\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "file1 = codecs.open('new_train.csv','r','utf-8') \n",
    "line = file1.read() \n",
    "words = line.split()\n",
    "print(\"poop\")\n",
    "appendFile = open('stopwords_train.csv','a', encoding='utf-8') \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile.write(r)\n",
    "    appendFile.write(\"\\n\")\n",
    "appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b62982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# import csv,numpy \n",
    "# from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "# reader = csv.reader(open('Medium_Edited.csv', 'rU'), delimiter= \",\",quotechar='|')\n",
    "# tokenData = word_tokenize(reader)\n",
    "\n",
    "# with open(\"Medium_Edited.csv\", \"rU\") as csvfile:\n",
    "# tokenData = nltk.word_tokenize(str(reader))\n",
    "\n",
    "# # for line in reader:\n",
    "# #     for field in line:\n",
    "# #         tokens = word_tokenize(field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca75fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
